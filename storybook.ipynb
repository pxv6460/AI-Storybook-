{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name for our LLMs.\n",
    "GEMINI_MODEL = \"gemini-1.5-pro-001\"\n",
    "# Store the API key in a variable.\n",
    "GEMINI_API_KEY = \"AIzaSyDnlYQXF4NwZzu1nhMCW39ex5PT7chhM2M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for agent with built-in tools.\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from diffusers import StableDiffusion3Pipeline, FluxPipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import random \n",
    "import re\n",
    "\n",
    "#from langchain.tools import Imagegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1: One day, a pirate named One-Eyed Jack was on a quest to find the legendary treasure of Captain <noun>. Legend said it was buried on a secret island populated by <adjective> monkeys.\n",
      "\n",
      "Scene 2: After weeks at sea, Jack and his crew finally found the island. They <verb> off the ship and into the jungle, following a treasure map that Jack had stolen from a <noun> he'd befriended at a tavern.\n",
      "\n",
      "Scene 3: Deep in the jungle, they found the treasure buried under a giant palm tree.  They <adverb> dug it up, only to find that it was just a chest full of bananas! One-Eyed Jack couldn't help but <verb> at the absurdity of it all. \n",
      "\n",
      "['<noun>', '<adjective>', '<verb>', '<noun>', '<adverb>', '<verb>']\n",
      "Let's fill in your Mad Lib story!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a <noun>:  BILL THE jacked rhino\n",
      "Enter a <adjective>:  silly\n",
      "Enter a <verb>:  running\n",
      "Enter a <noun>:  jake the kite\n",
      "Enter a <adverb>:  upbeat\n",
      "Enter a <verb>:  singing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BILL THE jacked rhino', 'silly', 'running', 'jake the kite', 'upbeat', 'singing']\n",
      "\n",
      "Here is your completed Mad Lib story:\n",
      "\n",
      "Scene 1: One day, a pirate named One-Eyed Jack was on a quest to find the legendary treasure of Captain BILL THE jacked rhino. Legend said it was buried on a secret island populated by silly monkeys.\n",
      "\n",
      "Scene 2: After weeks at sea, Jack and his crew finally found the island. They running off the ship and into the jungle, following a treasure map that Jack had stolen from a jake the kite he'd befriended at a tavern.\n",
      "\n",
      "Scene 3: Deep in the jungle, they found the treasure buried under a giant palm tree.  They upbeat dug it up, only to find that it was just a chest full of bananas! One-Eyed Jack couldn't help but singing at the absurdity of it all. \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cee8354ec7041b6abe4d1aee066f809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8f01353ca44841b98fe767b4f2864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387fc5beabc04f74811a727043a35296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image for Scene 1 saved as: scene_1.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338c8b0adefd4a979f4223f8201872ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image for Scene 2 saved as: scene_2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e078ebfd8f34600a0c8aa12c0482544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image for Scene 3 saved as: scene_3.png\n"
     ]
    }
   ],
   "source": [
    "def generate_madlib_prompt1():\n",
    "    \"\"\"Generates a random Mad Lib prompt with placeholders and varied themes.\"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.7)\n",
    "    try:\n",
    "        themes = [\n",
    "            \"a detective solving a quirky mystery\",\n",
    "            \"a space explorer discovering a new planet\",\n",
    "            \"a pirate on a treasure hunt\",\n",
    "            \"a chef trying to win a cooking competition\",\n",
    "            \"a superhero fighting an unusual villain\",\n",
    "            \"a wizard casting spells in a magical land\"\n",
    "            \"a dog lost his ball in the park\" \n",
    "        ]\n",
    "        theme = random.choice(themes)\n",
    "\n",
    "        # Prompt definition with a dynamic theme\n",
    "        prompt = f\"\"\"Write a 120-150 word humorous Mad Lib story in the theme of {theme}.\n",
    "        Include at least 6 placeholders enclosed in <>: 2 nouns, 2 verbs, one adverb, and one adjective. \n",
    "        Separate the story into 3 distinct scenes with transitions. \n",
    "        Format the output as: \n",
    "        Scene 1: [scene 1 content]\"\"\"\n",
    "        \n",
    "        # LLM response\n",
    "        response = llm.predict(prompt)\n",
    "        print(response)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating prompt: {e}\")\n",
    "        return \"Scene 1: {fallback scene}. Scene 2: {fallback scene}. Scene 3: {fallback scene}.\"\n",
    "\n",
    "def play_madlib1(prompt):\n",
    "    \"\"\"Plays the Mad Lib game with the given prompt.\"\"\"\n",
    "    placeholders = re.findall(r\"\\<[^\\>]+\\>\", prompt)  # Extract placeholders dynamically\n",
    "    print(placeholders)\n",
    "    words = []\n",
    "\n",
    "    print(\"Let's fill in your Mad Lib story!\")\n",
    "\n",
    "    for placeholder in placeholders:\n",
    "        user_input = input(f\"Enter a {placeholder}: \")\n",
    "        words.append(user_input)\n",
    "    print(words)\n",
    "    word_iter = iter(words)\n",
    "    def replace_match(match):\n",
    "        try:\n",
    "            return next(word_iter)\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Not enough words to replace all placeholders\")\n",
    "    try:\n",
    "        # Format the story with the user's inputs\n",
    "        madlib_story = re.sub(r\"<.*?>\", replace_match, prompt)\n",
    "        #madlib_story = clean_merged_words(madlib_story)\n",
    "        print(\"\\nHere is your completed Mad Lib story:\\n\")\n",
    "        print(madlib_story)\n",
    "        return madlib_story\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing word for placeholder {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_scenes(madlib_story):\n",
    "    \"\"\"Extract scenes from the Mad Lib story.\"\"\"\n",
    "    scenes = []\n",
    "    current_scene = []\n",
    "\n",
    "    for line in madlib_story.splitlines():\n",
    "        if line.strip().startswith(\"Scene\"):  # Detect scene headers\n",
    "            if current_scene:\n",
    "                scenes.append(\" \".join(current_scene).strip())\n",
    "                current_scene = []\n",
    "        current_scene.append(line)\n",
    "\n",
    "    if current_scene:  # Append the last scene\n",
    "        scenes.append(\" \".join(current_scene).strip())\n",
    "\n",
    "    return scenes\n",
    "    \n",
    "def generate_images1(scenes):\n",
    "    \"\"\"Generates images for each scene in the story.\"\"\"\n",
    "    try:\n",
    "        # Initialize the pipeline\n",
    "        #pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "        #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        #pipe = pipe.to()\n",
    "        #print(f\"Model loaded on {device}\")\n",
    "        pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "        pipe.enable_model_cpu_offload()\n",
    "\n",
    "        for i, scene in enumerate(scenes, start=1):\n",
    "            prompt = f\"Cartoon style: {scene}. Keep characters, background and artstyle consistent across scenes.\"\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=20,\n",
    "                height=400,\n",
    "                width=400,\n",
    "                guidance_scale=7.5, generator=torch.manual_seed(42)).images[0]\n",
    "\n",
    "            image_path = f\"scene_{i}.png\"\n",
    "            image.save(image_path)\n",
    "            print(f\"Image for Scene {i} saved as: {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error Traceback:\")\n",
    "        \n",
    "prompt = generate_madlib_prompt1()\n",
    "story = play_madlib1(prompt)\n",
    "scenes = extract_scenes(story)\n",
    "\n",
    "generate_images1(scenes)\n",
    "#generate_image_for_story_flux(scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### old code\n",
    "#def generate_madlib_prompt():\n",
    "#    \"\"\"Generates a random Mad Lib prompt.\"\"\"\n",
    "#    try:\n",
    "#        # Initialize the LLM\n",
    "#        llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.8)\n",
    "#\n",
    "#        # Define the prompt for the LLM\n",
    "#        prompt = \"\"\" Give your output in plain text, without markdown. Write a 150-200 word, humorous Mad Lib story with 12 blanks. \n",
    "#        The blanks should be for 3 nouns (noun1, noun2, noun3), 3 verbs (verb1, verb2, verb3), \n",
    "#        3 adjectives (adjective1, adjective2, adjective3), and 3 adverbs (adverb1, adverb2, adverb3). Take the following Mad Lib and break it into 3 scenes\n",
    "#        Identify where transitions happen based on changes in topic, setting, or action. Seperate Paragraphs: {paragraph}\\n\\n Split the paragraph \n",
    "#        into scenes and explain the transitions.\"\"\"\n",
    "#\n",
    "#        # Get the response from the LLM\n",
    "#        response = llm.predict(text=prompt)\n",
    "#        \n",
    "#        print(f\"LLM raw response: {response}\")\n",
    "#\n",
    "#        modified_response = response.replace(\"[\", \"{\").replace(\"]\", \"}\").replace(\"_\", \"\")\n",
    "#        return modified_response  # Return the story with placeholders\n",
    "#    except Exception as e:\n",
    "#        # Fallback Mad Lib prompt in case of errors\n",
    "#        print(f\"LLM generation failed. Error: {e}\")\n",
    "#        return \"Yo, here's your fallback story. What an adventure it was!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### old code\n",
    "#def play_madlib(prompt):\n",
    "#    \"\"\"Plays the Mad Lib game with the given prompt.\"\"\"\n",
    "#   # Gather user inputs for the placeholders\n",
    "#    words = {\n",
    "#       \"adjective1\": input(\"Enter an adjective: \"),\n",
    "#       \"adjective2\": input(\"Enter another adjective: \"),\n",
    "#       \"adjective3\": input(\"Enter a third adjective: \"),\n",
    "#       \"noun1\": input(\"Enter a noun: \"),\n",
    "#       \"noun2\": input(\"Enter another noun: \"),\n",
    "#       \"noun3\": input(\"Enter a third noun: \"),\n",
    "#       \"adverb1\": input(\"Enter an adverb: \"),\n",
    "#       \"adverb2\": input(\"Enter another adverb: \"),\n",
    "#       \"adverb3\": input(\"Enter a third adverb: \"),\n",
    "#       \"verb1\": input(\"Enter a verb: \"),\n",
    "#       \"verb2\": input(\"Enter another verb: \"),\n",
    "#       \"verb3\": input(\"Enter a third verb: \"),\n",
    "#    }\n",
    "#\n",
    "#    # Replace placeholders in the prompt with the user's words\n",
    "#    try:\n",
    "#        madlib_story = prompt.format(**words)\n",
    "#        print(\"\\nHere is your completed Mad Lib story:\\n\")\n",
    "#        print(madlib_story)\n",
    "#        return madlib_story\n",
    "#    except KeyError as e:\n",
    "#        print(f\"Error: Missing word for placeholder {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code\n",
    "#def generate_image_for_story_stable(madlib_story):\n",
    "#    \"\"\"generates the images of every scene of the madlib\"\"\"\n",
    "#    ## gather the scenes from the madlib \n",
    "#    #try:\n",
    "#    #    image_gen = VertexAIImageGeneratorChat(api_key=GEMINI_API_KEY)\n",
    "#    #    \n",
    "#    #    #scenes = [s for s in scenes if s]\n",
    "#    #    scenes = madlib_story.split(\". \")\n",
    "#    #    for i, scene in enumerate(scenes, start = 1):\n",
    "#    #        if scene.strip():\n",
    "#    #            print(f\"printing image for scene {i}\")\n",
    "#    #            messages = [HumanMessage(content=[f\"create a humorous image of the following scene: {scene.strip()} keep the iamges consistant\"])]\n",
    "#    #            response = image_gen.invoke(messages)\n",
    "#    #            print(f\"Scene {i} image generated: {response['image_url']}\")\n",
    "#    #except Exception as e:\n",
    "#    #    print(f\"Error generating images: {e}\")\n",
    "#    try:\n",
    "#        pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "#        pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "#        scenes = madlib_story.split(\". \")\n",
    "#        for i, scene in enumerate(scenes, start = 1):\n",
    "#            if scene.strip():\n",
    "#                #print(f\"printing image for scene {i}\")\n",
    "#                #prompt = (f\"create a image of the following scenes in the artstyle of cartoon: {scene.strip()} keep the images consistant by having the same time of image so the characters look the same scene to scene dont include any text in the scene\")            \n",
    "#                ## Generate the image\n",
    "#                #image = pipe(prompt).images[0]\n",
    "#                ## Save the image\n",
    "#                #image_path = f\"scene_{i}.png\"\n",
    "#                #image.save(image_path)\n",
    "#                #print(f\"Scene {i} image saved to: {image_path}\")\n",
    "#                image = pipe(\n",
    "#                prompt=f\" create a image with no text in the image of the following scenes in the artstyle of cartoon: {scene.strip()} keep the images characters and style consistant\",\n",
    "#                negative_prompt=\"\",\n",
    "#                num_inference_steps=28,\n",
    "#                height=1024,\n",
    "#                width=1024,\n",
    "#                guidance_scale=7.0,\n",
    "#                ).images[0]\n",
    "#\n",
    "#                # save images and display them \n",
    "#                image_path = f\"scene_{i}.png\"\n",
    "#                image.save(image_path)\n",
    "#                print(f\"the image {image_path} was saved the the folder\")\n",
    "#\n",
    "#                image.show()\n",
    "#    except Exception as e:\n",
    "#        print(f\"Error generating images: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = generate_madlib_prompt()  # Generate a random Mad Lib prompt\n",
    "#story = play_madlib(prompt)  # Play the Mad Lib game\n",
    "#generate_image_for_story_stable(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
