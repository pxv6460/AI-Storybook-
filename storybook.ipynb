{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.cloud import aiplatform\n",
    "#from google.cloud.aiplatform import Model\n",
    "#!gcloud auth login\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name for our LLMs.\n",
    "GEMINI_MODEL = \"gemini-1.5-pro-001\"\n",
    "# Store the API key in a variable.\n",
    "GEMINI_API_KEY = \"AIzaSyC7S4KhCBWW8mn8_exr9J-MMTCuTFvrnHM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for agent with built-in tools.\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from diffusers import StableDiffusion3Pipeline, FluxPipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "#from langchain.tools import Imagegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_madlib_prompt():\n",
    "    \"\"\"Generates a random Mad Lib prompt.\"\"\"\n",
    "    try:\n",
    "        # Initialize the LLM\n",
    "        llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.8)\n",
    "\n",
    "        # Define the prompt for the LLM\n",
    "        prompt = \"\"\"Write a 150-200 word, humorous Mad Lib story with 4 blanks. \n",
    "        The blanks should be for 1 noun (noun1), 1 verb (verb1), \n",
    "        1 adjective (adjective1), and 1 adverb (adverb1). Take the following Mad Lib and break it into 3 scenes\n",
    "        Identify where transitions happen based on changes in topic, setting, or action. Seperate Paragraphs: {paragraph}\\n\\n Split the paragraph \n",
    "        into scenes and explain the transitions: Give your output in plain text, without markdown.\"\"\"\n",
    "\n",
    "        # Get the response from the LLM\n",
    "        response = llm.predict(text=prompt)\n",
    "        \n",
    "        print(f\"LLM raw response: {response}\")\n",
    "\n",
    "        modified_response = response.replace(\"[\", \"{\").replace(\"]\", \"}\").replace(\"_\", \"\")\n",
    "        return modified_response  # Return the story with placeholders\n",
    "    except Exception as e:\n",
    "        # Fallback Mad Lib prompt in case of errors\n",
    "        print(f\"LLM generation failed. Error: {e}\")\n",
    "        return \"Yo, here's your fallback story. What an adventure it was!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_madlib(prompt):\n",
    "    \"\"\"Plays the Mad Lib game with the given prompt.\"\"\"\n",
    "   # Gather user inputs for the placeholders\n",
    "    words = {\n",
    "       \"adjective1\": input(\"Enter an adjective: \"),\n",
    "       \"noun1\": input(\"Enter a noun: \"),\n",
    "       \"adverb1\": input(\"Enter an adverb: \"),\n",
    "       \"verb1\": input(\"Enter a verb: \"),\n",
    "    }\n",
    "\n",
    "    # Replace placeholders in the prompt with the user's words\n",
    "    try:\n",
    "        madlib_story = prompt.format(**words)\n",
    "        print(\"\\nHere is your completed Mad Lib story:\\n\")\n",
    "        print(madlib_story)\n",
    "        return madlib_story\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing word for placeholder {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_for_story_stable(madlib_story):\n",
    "    \"\"\"generates the images of every scene of the madlib\"\"\"\n",
    "    ## gather the scenes from the madlib \n",
    "    #try:\n",
    "    #    image_gen = VertexAIImageGeneratorChat(api_key=GEMINI_API_KEY)\n",
    "    #    \n",
    "    #    #scenes = [s for s in scenes if s]\n",
    "    #    scenes = madlib_story.split(\". \")\n",
    "#\n",
    "    #    for i, scene in enumerate(scenes, start = 1):\n",
    "    #        if scene.strip():\n",
    "    #            print(f\"printing image for scene {i}\")\n",
    "    #            messages = [HumanMessage(content=[f\"create a humorous image of the following scene: {scene.strip()} keep the iamges consistant\"])]\n",
    "    #            response = image_gen.invoke(messages)\n",
    "    #            print(f\"Scene {i} image generated: {response['image_url']}\")\n",
    "    #except Exception as e:\n",
    "    #    print(f\"Error generating images: {e}\")\n",
    "    try:\n",
    "        pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "        pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        scenes = madlib_story.split(\". \")\n",
    "        for i, scene in enumerate(scenes, start = 1):\n",
    "            if scene.strip():\n",
    "                #print(f\"printing image for scene {i}\")\n",
    "                #prompt = (f\"create a image of the following scenes in the artstyle of cartoon: {scene.strip()} keep the images consistant by having the same time of image so the characters look the same scene to scene dont include any text in the scene\")            \n",
    "                ## Generate the image\n",
    "                #image = pipe(prompt).images[0]\n",
    "                ## Save the image\n",
    "                #image_path = f\"scene_{i}.png\"\n",
    "                #image.save(image_path)\n",
    "                #print(f\"Scene {i} image saved to: {image_path}\")\n",
    "                image = pipe(\n",
    "                prompt=f\" create a image with no text in the image of the following scenes in the artstyle of cartoon: {scene.strip()} keep the images characters and style consistant\",\n",
    "                negative_prompt=\"\",\n",
    "                num_inference_steps=28,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                guidance_scale=7.0,\n",
    "                ).images[0]\n",
    "\n",
    "                # save images and display them \n",
    "                image_path = f\"scene_{i}.png\"\n",
    "                image.save(image_path)\n",
    "                print(f\"the image {image_path} was saved the the folder\")\n",
    "\n",
    "                image.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating images: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_for_story_flux(madlib_story):\n",
    "    \"\"\"generates the images of every scene of the madlib\"\"\"\n",
    "    ## gather the scenes from the madlib \n",
    "    #try:\n",
    "    #    image_gen = VertexAIImageGeneratorChat(api_key=GEMINI_API_KEY)\n",
    "    #    \n",
    "    #    #scenes = [s for s in scenes if s]\n",
    "    #    scenes = madlib_story.split(\". \")\n",
    "#\n",
    "    #    for i, scene in enumerate(scenes, start = 1):\n",
    "    #        if scene.strip():\n",
    "    #            print(f\"printing image for scene {i}\")\n",
    "    #            messages = [HumanMessage(content=[f\"create a humorous image of the following scene: {scene.strip()} keep the iamges consistant\"])]\n",
    "    #            response = image_gen.invoke(messages)\n",
    "    #            print(f\"Scene {i} image generated: {response['image_url']}\")\n",
    "    #except Exception as e:\n",
    "    #    print(f\"Error generating images: {e}\")\n",
    "    try:\n",
    "        pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "        pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        scenes = madlib_story.split(\". \")\n",
    "        for i, scene in enumerate(scenes, start = 1):\n",
    "            if scene.strip():\n",
    "                #print(f\"printing image for scene {i}\")\n",
    "                #prompt = (f\"create a image of the following scenes in the artstyle of cartoon: {scene.strip()} keep the images consistant by having the same time of image so the characters look the same scene to scene dont include any text in the scene\")            \n",
    "                ## Generate the image\n",
    "                #image = pipe(prompt).images[0]\n",
    "                ## Save the image\n",
    "                #image_path = f\"scene_{i}.png\"\n",
    "                #image.save(image_path)\n",
    "                #print(f\"Scene {i} image saved to: {image_path}\")\n",
    "                image = pipe(\n",
    "                prompt=f\" create a image with no text in the image of the following scenes in the artstyle of cartoon: {scene.strip()} keep the images characters and style consistant\",\n",
    "                negative_prompt=\"\",\n",
    "                num_inference_steps=28,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                guidance_scale=7.0,\n",
    "                ).images[0]\n",
    "\n",
    "                # save images and display them \n",
    "                image_path = f\"scene_{i}.png\"\n",
    "                image.save(image_path)\n",
    "                print(f\"the image {image_path} was saved the the folder\")\n",
    "\n",
    "                image.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating images: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_35368\\1499822196.py:15: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(text=prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM raw response: Okay, here's your Mad Libs story:\n",
      "\n",
      "**Scene 1**\n",
      "\n",
      "One sunny afternoon, a little boy was playing with his favorite toy [noun1] in the backyard. He was having so much fun [verb1]ing it up and down, making airplane noises and giggling.\n",
      "\n",
      "**Transition:** The story transitions from the boy playing to him encountering his neighbor. This is a change in action.\n",
      "\n",
      "**Scene 2**\n",
      "\n",
      "Suddenly, his  [adjective1]  neighbor, Mrs. Picklebottom, peeked over the fence.  \n",
      "\n",
      "**Transition:** The story transitions from Mrs. Picklebottom's appearance to her reaction to the boy's play, which is a change in topic. \n",
      "\n",
      "**Scene 3**\n",
      "\n",
      "She cleared her throat [adverb1] and said, \"Young man, must you make so much noise?\"  The boy, startled, dropped his toy and ran inside. \n",
      "\n",
      "**Mad Libs Blanks:**\n",
      "\n",
      "* noun1: Noun (a toy)\n",
      "* verb1: Verb (action word)\n",
      "* adjective1: Adjective (describes the neighbor)\n",
      "* adverb1:  Adverb (describes how she cleared her throat) \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an adjective:  silly \n",
      "Enter a noun:  draco the orca\n",
      "Enter an adverb:  upbeat\n",
      "Enter a verb:  boxing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is your completed Mad Lib story:\n",
      "\n",
      "Okay, here's your Mad Libs story:\n",
      "\n",
      "**Scene 1**\n",
      "\n",
      "One sunny afternoon, a little boy was playing with his favorite toy draco the orca in the backyard. He was having so much fun boxinging it up and down, making airplane noises and giggling.\n",
      "\n",
      "**Transition:** The story transitions from the boy playing to him encountering his neighbor. This is a change in action.\n",
      "\n",
      "**Scene 2**\n",
      "\n",
      "Suddenly, his  silly   neighbor, Mrs. Picklebottom, peeked over the fence.  \n",
      "\n",
      "**Transition:** The story transitions from Mrs. Picklebottom's appearance to her reaction to the boy's play, which is a change in topic. \n",
      "\n",
      "**Scene 3**\n",
      "\n",
      "She cleared her throat upbeat and said, \"Young man, must you make so much noise?\"  The boy, startled, dropped his toy and ran inside. \n",
      "\n",
      "**Mad Libs Blanks:**\n",
      "\n",
      "* noun1: Noun (a toy)\n",
      "* verb1: Verb (action word)\n",
      "* adjective1: Adjective (describes the neighbor)\n",
      "* adverb1:  Adverb (describes how she cleared her throat) \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448e542bff354b219f00cc9c399c0471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22959c443a7f48abba18c3110c42a3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf1a6728a5f470295d94b0b58557958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image scene_1.png was saved the the folder\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_madlib_prompt()  # Generate a random Mad Lib prompt\n",
    "story = play_madlib(prompt)  # Play the Mad Lib game\n",
    "generate_image_for_story_stable(story)\n",
    "generate_image_for_story_flux(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using OpenAI \n",
    "#import openai\n",
    "#\n",
    "#def generate_madlib_prompt():\n",
    "#  \"\"\"Generates a random Mad Lib prompt.\"\"\"\n",
    "#\n",
    "#  prompt = \"Write a 150-200 word, humorous Mad Lib story with 12 blanks. The blanks should be for 3 nouns, 3 verbs, 3 adjectives, 3 adverbs the story can be as creative as you want it to be.\"\n",
    "#  response = openai.Completion.create(\n",
    "#    engine=\"gpt-4o\",\n",
    "#    prompt=prompt,\n",
    "#    max_tokens=100,\n",
    "#    n=1,\n",
    "#    temperature=0.7\n",
    "#  )\n",
    "#  return response.choices[0].text.strip()\n",
    "#\n",
    "#def play_madlib(prompt):\n",
    "#  \"\"\"Plays the Mad Lib game with the given prompt.\"\"\"\n",
    "#\n",
    "#  words = []\n",
    "#  for i in range(12):\n",
    "#    part_of_speech = input(f\"Enter word {i+1}: \")\n",
    "#    words.append(part_of_speech)\n",
    "#      \n",
    "#  # Replace placeholders in the prompt with the user's words\n",
    "#  madlib_story = prompt.format(*words)\n",
    "#\n",
    "#  print(madlib_story)\n",
    "#\n",
    "## Get a random Mad Lib prompt\n",
    "#prompt = generate_madlib_prompt()\n",
    "#\n",
    "## Play the Mad Lib game\n",
    "#play_madlib(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
